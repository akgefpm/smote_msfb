

generate summaries for the revised datasets

bibtex
corel
bookmarks
delicious
Enron
genbase
medical
polarity -> only one response here [NOT REFRESHED]
scale <- four different datasets and only 1 response [NOT REFRESHED]
subjectivity <- only one response here [NOT REFRESHED]

tmc2007 <- fix the target variable


Pending Steps:-

0. Setup different configs for different experiments - code this functionality
1. setup all the experimentation codes for all datasets

Find a more clean way of identifying the minority samples for resampling rather than hardcoded limits of 0.5 and 0.8. How about a S class function using the predicted probability. 

Key contribution:
1. A key contribution could be automatically resampling algorithm. Not requiring a lot of things to be done by the user.

### 4 core ideas - to be implemented ### 

2. Resampling is classification algorithm specific <- [Need to make modification for this. This is not without challenges. Probability predicted is dramatically different for different algorithms. For example, Trees can give much higher probability for the same minority sample (% of correct observations in the leaf node) hence the cut off will be diffcult to be defined. On the other hand, final metric is ROC AUC curve which is impacted across different probability cut offs.]  <- To be done later.

4. Identifying the samples to be resample also has to be classification algorithm specific. For the same dataset, the samples which would require resampling would be very different for LGBM, Logistic regression and Naive Bayes. The problem to be handled in this case is -> The prediction probabilities generated by the different algorithms are very different. In the sense, For the same dataset -> Logistic regression gives prediction probabilities across the spectrum where as Tree based models might give all prediction probabilities below 0.1

### Another potential idea ###

1. Increase / Decrease the no. of samples to be considered for neighbourhood definition to handle problem of small sub-clusters
2. Can you build resampling in the classification algorithm such as Neural Network and optimize the whole thing using stochastic gradient descent.

2. Reporting of the dataset characterstics -  All data folders will have a file named processed_dataset.parquet which will be processed and aggregated metrics stored. Data summary code 
    0) Domain of dataset
    a) No. of rows
    b) No. of columns
    c) Multi Response / Single Response
    d) Imbalance ratio
    e) Good hub and bad hub metrics    


/repos/smote_msfb/helper_functions/

/domino/datasets/local/smote_msfb/tencent_data/




COMPLETED ITEMS:

3. Cross validation to determine the best level of resampling <- To be done later. [This piece will come in handy to handles cases with severe immbalance and very small minority samples datasets.] -----> COMPLETED

5. Removing noise from the minority samples before identifying the samples for resamples is also very important -> If we generate synthetic samples from samples which are noise it may add more noise to the dataset only. Use Isolation Forest for identifying the samples which are noise/outliers and do NOT use them for synthetic sample generation. -----> COMPLETED

6. As we are using anamoly detection on the minority sample, there is no need to do a post prediction noise removal (minority samples with very low prediction probability) from the dataset. 



1. A very simple but completely different idea would be:
    a) Purify the seeds
    b) Find appropriate majority sample <- this is based on the classficiation algorithm used. Different algorithms will have different majority samples identified. [undersampling piece]
        -> 
    c) Use CV to determine the level of resampling


In my custom smote_msfb algorithm for high dimensional imbalanced classification algorithm, One of the key aspects is finding appropriate minority samples for resampling. The function get_minority_obs_to_resample(). In the function I am basically using a bagged classifier Logistic regression for now to predict probabilities of minority samples. Then filtering the predicted probabilities of the minority sample at 0.5 and 0.8 limits to identify samples which would benefit from resampling. Thus focussing the resampling on correct minority samples. 

These hardcoded limits have two problems: a) The pred prob range from different algorithms could be very different for example - Logistic regression and Decision trees can have very different limits
b) Many a time I get stuck in cases where the limits lead to no minority sample identified for resampling. 

Are there any better ways to identify the correct minority samples for resampling?